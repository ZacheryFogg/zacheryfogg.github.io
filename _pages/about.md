---
layout: about
title: about
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info:

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

Hi, I'm Zach! I earned my B.S. in computer science at the University of Vermont (home state). 
At UVM, I conducted applied machine learning research under the guidance of [Prof. Byung Lee](https://bslee.w3.uvm.edu/) and [Prof. Chris Skalka](https://ceskalka.w3.uvm.edu/). 
I worked on methods for modeling multivariate time-series data.

Although currently working in industry as a backend software engineer, I am applying to several CS PhD programs for Fall 2024 to pursue a career in research.

I believe ML holds amazing potential to simplify the communication between humans and machines and enable us to leverage technology more effectively in our day-to-day lives. Natural language is the primary medium by which humans communicate ideas, and so it follows that models that can interpret and express themselves in natural language will be paramount in allowing people to efficiently interact with and direct machines. 

With these motivations, my research interests lie at the intersection of NLP and ML with the goal of creating language models (LMs)
that can effectively understand and communicate with humans in natural language. The current topics
that I am most drawn to are the following: 

`(1) Memory Augmentation and External Knowledge Retrieval` I believe that augmenting LMs with updateable long-term memory (parametric or non-parametric) or external
knowledge retrieval will be essential in continuing to improve their abilities by decoupling the scaling of knowledge
that they have access to from their training data and model size, as well as improving their interpretability and
factuality. 

`(2) Multimodal Learning` I think that models that have the ability to learn from and interpret
multiple modalities of data will be able to build much richer representations of language than models that rely on
textual data alone. 

`(3) Representation Learning` Improving the mechanisms available to LMs for automatically
learning semantically and contextually aware representastions (whether those representations be of words, phrases, or
even concepts) will be crucial in bolstering their understanding of natural language.


<!-- 
Write your biography here. Tell the world about yourself.  

Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](https://fontawesome.com/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->